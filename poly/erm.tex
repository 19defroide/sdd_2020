%-*- coding: iso-latin-1 -*-
\label{chap:erm}

\todo{
  \begin{itemize}
  \item Cohérence des notations avec les chapitres précédents.
  \item Lien avec le cours d'optimisation.
  \item Alléger / élaguer.
  \end{itemize}
}

\paragraph{Notions :} classification, régression, espace des hypothèses,
minimisation du risque empirique, moindres carrés, modèles paramétriques
linéaires
\paragraph{Objectifs pédagogiques :} 
\begin{itemize}      
  \setlength{\itemsep}{3pt}
\item Formaliser un problème d'apprentissage supervisé.
\item Décrire l'espace des hypothèses dans le cas d'un modèle paramétrique.
\item Prouver l'équivalence entre maximisation de la vraisemblance et
  minimisation du risque empirique dans le cas gaussien.
\item Mettre en \oe{}uvre une régression linéaire.
\end{itemize}



Nous nous intéressons maintenant aux problèmes d'apprentissage {\it supervisé}
: il s'agit de développer des algorithmes qui soient capables d'apprendre des
modèles {\it prédictifs}. À partir d'exemples étiquetés, ces modèles seront
capables de prédire l'étiquette de nouveaux objets. Le but de ce chapitre est
de développer les concepts généraux qui nous permettent de formaliser ce type
de problèmes.


% \paragraph{Compétences}


\section{Formalisation d'un problème d'apprentissage supervisé}
\label{sec:sup_learn}
Un problème d'\textit{apprentissage supervisé} peut être formalisé de la façon
suivante : étant données $n$ {\it observations}
$\{\xx^1, \xx^2, \dots, \xx^n\}$, où chaque observation $\xx^i$ est un élément
de l'espace des observations $\XX$, et leurs {\it étiquettes}
$\{y^1, y^2, \dots, y^n\}$, où chaque étiquette $y^i$ appartient à l'espace des
étiquettes $\YY$, le but de l'apprentissage supervisé est de trouver une
fonction $f: \XX \rightarrow \YY$ telle que $f(\xx) \approx y,$ pour toutes les
paires $(\xx, y) \in \XX \times \YY$ ayant la même relation que les paires
observées. L'ensemble de $\DD = \{(\xx^i, y^i)\}_{i=1, \dots, n}$ forme le
\textit{jeu d'apprentissage}.

Nous allons considérer dans ce cours deux cas particuliers pour $\YY:$
\begin{itemize}
\item $\YY = \RR :$ on parle d'un problème de {\it régression} ;
\item $\YY = \{0, 1\} :$ on parle d'un problème de {\it classification
    binaire}, et les observations dont l'étiquette vaut $0$ sont appelées {\it
    négatives} tandis que celles dont l'étiquette vaut $1$ sont appelées {\it
    positives}. Dans certains cas, il sera mathématiquement plus simple
  d'utiliser $\YY = \{-1, 1\}$ ;
% \item $\YY=\{1, 2, \dots, C\},\; C>2 : $ on parle d'un problème de {\it
%     classification multi-classe}.
\end{itemize}

Dans de nombreuses situations, on se ramènera au cas où $\XX = \RR^p.$ On dira
alors que les observations sont représentées par $p$ {\it variables}.  Dans ce
cas, la matrice $X \in \RR^{n \times p}$ telle que $X_{ij} = x^i_j$ soit la
$j$-ème variable de la $i$-ème observation est appelée {\it matrice de données}
ou {\it matrice de design}.
  
Le machine learning étant issu de plusieurs disciplines et champs
d'applications, on trouvera plusieurs noms pour les mêmes objets.  Ainsi les
variables sont aussi appelées {\it descripteurs}, {\it attributs}, {\it
  prédicteurs}, ou {\it caractéristiques} (en anglais, {\it variables,
  descriptors, attributes, predictors} ou encore {\it features}).  Les {\it
  observations} sont aussi appelées {\it exemples}, {\it échantillons} ou {\it
  points du jeu de données} (en anglais, {\it samples} ou {\it data
  points}). Enfin, les étiquettes sont aussi appelées {\it variables cibles}
(en anglais, {\it labels, targets} ou {\it outcomes}).

% Ces concepts sont illustrés sur la figure~\ref{fig:suplearning}.

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.7\textwidth]{figures/erm/suplearning}
%   \caption{Les données d'un problème d'apprentissage supervisé sont organisées
%     en une matrice de design et un vecteur d'étiquettes. Les observations sont
%     représentées par leurs variables explicatives.}
%   \label{fig:suplearning}
% \end{figure}

\subsection{Décision}
Dans le cas d'un problème de classification, le modèle prédictif peut prendre
directement la forme d'une fonction $f$ à valeurs dans $\{0, 1\}$, ou utiliser
une fonction intermédiaire $g$ à valeurs réelles, qui associe à une observation
un score d'autant plus élevé qu'elle est susceptible d'être positive. Ce score
peut par exemple être la probabilité que cette observation appartienne à la
classe positive. On obtient alors $f$ en {\it seuillant} $g$ ; $g$ est appelée
{\it fonction de décision}.

% Dans le cadre d'un problème de classification binaire, on appelle {\it fonction
%   de décision}, ou {\it fonction discriminante}, une fonction
% $g:\XX \mapsto \RR$ telle que $f(\xx) = 0$ si et seulement si $g(\xx) \leq 0$
% et $f(\xx) = 1$ si et seulement si
% $g(\xx) > 0$. \\

% % Cette définition se généralise dans le cas de la classification {\it
% %   multi-classe} : on a alors $C$ fonctions de décision $g_c:\XX \mapsto \RR$
% % telles que $f(\xx) = \argmax_{c = 1, \dots, C} g_c(\xx).$

% Le concept de fonction de décision permet de partitionner l'espace en {\it
%   régions de décision} : \\
% Dans le cas d'un problème de classification binaire, la fonction discriminante
% partitionne l'espace des observations $\XX$ en deux {\it régions de décision},
% $\Rcal_0$ et $\Rcal_1$, telles que
% \begin{equation*}
%   \Rcal_0 = \{\xx \in \XX | g(\xx) \leq 0\} \text{ et }
%   \Rcal_1 = \{\xx \in \XX | g(\xx) > 0\}.
% \end{equation*}
% % Dans le cas multi-classe, on a alors $C$ régions de décision
% % \begin{equation*}
% %   \Rcal_c = \{\xx \in \XX | g_c(\xx) = \max_k g_k(\xx) \}.
% % \end{equation*}

% Les régions de décision sont séparées par des {\it frontières de décision} : \\
% Dans le cadre d'un problème de classification, on appelle {\it frontière de
%   décision}, ou {\it discriminant}, l'ensemble des points de $\XX$ où une
% fonction de décision s'annule.  Dans le cas d'un problème binaire, il y a
% une seule frontière de décision ; dans le cas d'un problème multi-classe à
% $C$ classes, il y en a $C$.

\section{Espace des hypothèses}
Pour poser un problème d'apprentissage supervisé, il nous faut décider du
type de fonctions de modélisation que nous allons considérer. 

On appelle {\it espace des hypothèses} l'espace de fonctions
$\FF \subseteq \YY^\XX$ décrivant les fonctions de modélisation que nous allons
considérer. Cet espace est choisi en fonction de nos {\it convictions} par
rapport au problème. 

\begin{exemple}
  Dans l'exemple de la figure~\ref{fig:simple_classif_pb}, on pourra décider de
  se restreindre à des discriminants qui soient des ellipses à axes parallèles
  aux axes de coordonnées.  Ainsi, l'espace des hypothèses sera
  \begin{equation*}
    \FF = \{ \xx \mapsto \alpha (x_1-a)^2 + \beta (x_2-b)^2 - 1  \}.
  \end{equation*}
\end{exemple}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/erm/simple_classif}
  \caption{Les exemples positifs (+) et négatifs (x) semblent être séparables
    par une ellipse.}
  \label{fig:simple_classif_pb}
\end{figure}

Étant donnés un jeu de $n$ observations étiquetées
$\DD = \{(\xx^i, y^i)\}_{i=1, \dots, n}$ et un espace d'hypothèses $\FF$.  La
tâche d'apprentissage supervisé consiste à supposer que les étiquettes $y^i$
ont été calculées grâce à une fonction $\phi: \XX \rightarrow \YY$, et à
trouver une hypothèse $f \in \FF$ qui approche au mieux la fonction cible
$\phi$.  Pour réaliser une telle tâche, nous allons avoir alors besoin de deux
outils supplémentaires :
\begin{enumerate}
\item Une façon de {\it quantifier la qualité d'une hypothèse}, afin de
  pouvoir déterminer si une hypothèse satisfaisante (voire optimale) a été
  trouvée.  Pour cela, nous allons définir dans la section~\ref{sec:losses}
  la notion de {\it fonction de coût}.
\item Une façon de {\it chercher une hypothèse optimale} dans $\FF$.  Dans
  cet ouvrage, nous allons nous concentrer sur les méthodes
  d'\textit{apprentissage par optimisation} : les algorithmes d'apprentissage
  supervisé que nous allons étudier auront pour but de trouver dans $\FF$
  l'hypothèse optimale au sens de la fonction de coût
  (cf. section~\ref{sec:mre}). Différents algorithmes définiront différents
  $\FF$, et selon les cas cette recherche sera exacte ou approchée.
\end{enumerate}

Le choix de l'espace des hypothèses est fondamental.  En effet, si cet espace
ne contient pas la \og bonne \fg~fonction % , par exemple si l'on choisit comme
% espace des hypothèses pour les données de la figure~\ref{fig:simple_classif_pb}
% l'ensemble des droites,
il sera impossible de trouver une bonne fonction de
décision.  Cependant, si l'espace est trop générique, il sera plus difficile et
intensif en temps de calcul d'y trouver une bonne fonction de modélisation.
  

\section{Minimisation du risque empirique}
\label{sec:mre}
Résoudre un problème d'apprentissage supervisé revient à trouver une fonction
$f \in \FF$ dont les prédictions soient les plus proches possibles des
véritables étiquettes, sur tout l'espace $\XX$. On utilise pour formaliser cela
la notion de {\it fonction de coût} :

Une {\it fonction de coût} $L: \YY \times \YY \rightarrow \RR$, 
aussi appelée {\it fonction de perte} ou {\it fonction d'erreur}
(en anglais : {\it cost function} ou {\it loss function})
est une fonction utilisée pour quantifier la qualité d'une prédiction : 
$L(y, f(\xx))$ est d'autant plus grande que l'étiquette $f(\xx)$ est éloignée de
la vraie valeur $y$.


Étant donnée une fonction de coût $L$, nous cherchons donc $f$ qui minimise
ce coût sur l'ensemble des valeurs possibles de $\xx \in \XX$, ce qui est
formalisé par la notion de {\it risque.}

Dans le cadre d'un problème d'apprentissage supervisé, on appelle {\it
  risque} l'espérance d'une fonction de coût :
\begin{equation*}
  \Rcal(h) = \EE_{\XX}[L(h(\xx), y)].
\end{equation*}

La fonction $f$ que nous cherchons vérifie donc $f = \argmin_{h \in \FF}
\EE[L(h(\xx), y)].$ Ce problème est généralement insoluble sans plus
d'hypothèses : si nous connaissions les étiquettes de tous les points de
$\XX$, nous n'aurions pas besoin d'apprentissage automatique.  Étant données
$n$ observations étiquetées $\{(\xx^i, y^i)\}_{i=1, \dots, n}$, on approchera
donc le risque par son estimation sur ces données observées

Dans le cadre d'un problème d'apprentissage supervisé, étant données $n$
observations étiquetées $\{(\xx^i, y^i)\}_{i=1, \dots, n}$, on appelle {\it risque
  empirique} l'estimateur
\begin{equation*}
  R_n(h) = \frac{1}{n} \sum_{i=1}^n L(h(\xx^i), y^i).
\end{equation*}


Le prédicteur par {\it minimisation du risque empirique} est donc
\begin{equation}
  \label{eq:erm}
  f = \argmin_{h \in \FF} \frac{1}{n} \sum_{i=1}^n L(h(\xx^i), y^i).
\end{equation}

Selon le choix de $\FF$, l'équation~\ref{eq:erm} peut avoir une solution
analytique explicite. Cela ne sera pas souvent le cas ; cependant on choisira
souvent une fonction de coût {\it convexe} afin de résoudre plus facilement ce
problème d'optimisation.

La minimisation du risque empirique est généralement un problème {\it mal posé}
au sens de Hadamard, c'est-à-dire qu'il n'admet pas une solution unique
dépendant de façon continue des conditions initiales. Il se peut par exemple
qu'un nombre infini de solutions minimise le risque empirique à zéro (voir
figure~\ref{fig:multiple_solutions}).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/erm/multiple_solutions}
  \caption{Une infinité de droites séparent parfaitement les points positifs
    (+) des points négatifs (x). Chacune d'entre elles a un risque empirique
    nul.}
  \label{fig:multiple_solutions}
\end{figure}
\todo{Non-convergence de l'ERM}
% De plus, le prédicteur par minimisation du risque empirique n'est pas
% statistiquement consistant. Rappelons qu'un estimateur $\theta_n$ (dépendant de
% $n$ observations) d'un paramètre $\theta$ est {\it consistant} s'il converge en
% probabilité vers $\theta$ quand $n$ croit vers l'infini :
% \begin{equation}
%   \label{eq:statistical_consistency}
%   \forall \epsilon > 0, \lim_{n \rightarrow \infty} 
%   \PP(|\theta_n - \theta| \geq \epsilon) = 0.
% \end{equation}

% La loi des grands nombres nous garantit que le risque empirique converge vers
% le risque :
% \begin{equation}
%   \label{eq:risk_cvg}
%   \forall h \in \FF, R_n(h) \xrightarrow[n \rightarrow \infty]{} \Rcal(h).
% \end{equation}
% Cela ne suffit cependant pas à garantir que le minimum du risque empirique
% $\min_{h \in \FF} R_n(h)$ converge vers le minimum du risque. En effet, si
% $\FF$ est l'espace des fonctions mesurables, $\min_{h \in \FF} R_n(h)$ vaut
% généralement $0$, ce qui n'est pas le cas de $\Rcal(h).$ Il n'y a donc aucune
% garantie que la fonction $f_n$ qui minimise $R_n(h)$ soit un bon estimateur du
% minimiseur $f$ de $\Rcal(h)$.

% La consistance de la minimisation du risque empirique dépend de l'espace des
% versions $\FF$. L'étude de cette consistance est un des principaux éléments de
% la théorie de l'apprentissage de Vapnik-Chervonenkis, qui dépasse largement le
% cadre de ce cours.


  
\section{Fonctions de coût}
\label{sec:losses}
Il existe de nombreuses fonctions de coût. Le choix d'une fonction de coût
dépend d'une part du problème en lui-même, autrement dit de ce que l'on trouve
pertinent pour le cas pratique considéré, et d'autre part de considérations
pratiques : peut-on ensuite résoudre le problème d'optimisation qui résulte de
ce choix de façon suffisamment exacte et rapide~?%  Cette section présente les
% fonctions de coûts les plus couramment utilisées et on pourra s'y référer tout
% au long de la lecture de cet ouvrage.

% \subsection{Fonctions de coût pour la classification binaire}
% Pour définir des fonctions de coût pour la classification binaire, on
% considèrera souvent $\YY = \{-1, 1\}$. En effet, dans le cas d'une
% classification parfaite, le produit $y f(\xx)$ est alors égal à $1$.

\subsection{Coût 0/1 pour la classification binaire}
Dans le cas d'une fonction $f$ à valeurs binaires, on appelle {\it fonction de
  coût 0/1}, ou {\it 0/1 loss}, la fonction suivante :
\begin{align*}
  L_{0/1} : \YY \times \YY & \rightarrow \RR \\
  y, f(\xx) & \mapsto
              \begin{cases}
                1 & \mbox{ si } f(\xx) \neq y \\
                0 & \mbox{ sinon.}
              \end{cases}
\end{align*}

En utilisant $\YY = \{-1, 1\}$, on peut la réécrire de la manière suivante :
\begin{equation*}
  L_{0/1}(y, f(\xx)) = \frac{1 - y f(\xx)}{2}.
\end{equation*}
Quand on utilise cette fonction de coût, le risque empirique est le nombre
moyen d'erreurs de prédiction.% \\

% Si l'on considère pour $f$ une fonction de décision (à valeurs réelles) plutôt
% qu'une fonction de prédiction à valeurs binaires, on peut définir la fonction
% de coût 0/1 comme suit :
% \subsubsection{Coût 0/1 pour la régression}
% Quand on considère une fonction de décision à valeurs réelles, on appelle
% {\it fonction de coût 0/1}, ou {\it 0/1 loss}, la fonction suivante :
% \begin{align*}
%   L_{0/1} : \YY \times \RR & \rightarrow \RR \\
%   y, f(\xx) & \mapsto
%               \begin{cases}
%                 1 & \mbox{ si } y f(\xx) \leq 0 \\
%                 0 & \mbox{ sinon.}
%               \end{cases}
% \end{align*}

% L'inconvénient de cette fonction de coût est qu'elle n'est pas dérivable, ce
% qui compliquera les problèmes d'optimisation l'utilisant. De plus, elle n'est
% pas très fine : l'erreur est la même que $f(\xx)$ soit très proche ou très loin
% du seuil de décision.  Rappelons que pour une classification parfaite, quand
% $\YY = \{-1, 1\}$, $y f(\xx) = 1$. On peut ainsi définir une fonction de coût
% qui soit d'autant plus grande que $y f(\xx)$ s'éloigne de $1$ à gauche ; on
% considère qu'il n'y a pas d'erreur si $y f(\xx) > 1$. Cela conduit à la
% définition d'erreur {\it hinge}, ainsi appelée car elle forme un coude, ou une
% charnière (cf. figure~\ref{fig:classif_losses}).

% \subsubsection{Erreur hinge}
% \label{sec:hinge-loss}
% On appelle {\it fonction d'erreur hinge}, ou {\it hinge loss}, la fonction
% suivante :
% \begin{align*}
%   L_{\text{hinge}} : \{-1, 1\} \times \RR & \rightarrow \RR \\
%   y, f(\xx) & \mapsto
%               \begin{cases}
%                 0 & \mbox{ si } y f(\xx) \geq 1 \\
%                 1 - y f(\xx) & \mbox{ sinon.}
%               \end{cases}
% \end{align*}
% De manière plus compacte, l'erreur hinge peut aussi s'écrire 
% \begin{equation*}
%   \label{eq:hinge-loss}
%   L_{\text{hinge}}(f(\xx), y) = \max\left(0, 1 - y f(\xx) \right) = \left[ 1 - y f(\xx)\right]_+. 
% \end{equation*}

% On peut aussi considérer que $f(\xx)$ doit être la plus proche possible de $1$
% pour les observations positives (et $-1$ pour les observations
% négatives). Ainsi, on pénalisera aussi les cas où $y f(\xx)$ s'éloigne de $1$
% par la droite, ce que l'on peut faire avec le {\it coût quadratique}.

% \subsubsection{Coût quadratique pour la classification binaire}
% Dans le cadre d'un problème de classification binaire, on appelle {\it coût
%   quadratique}, ou {\it square loss}, la fonction suivante :
% \begin{align*}
%   L_{\text{square}} : \{-1, 1\} \times \RR & \rightarrow \RR \\
%   y, f(\xx) & \mapsto  \left( 1 -y f(\xx)\right)^2.
% \end{align*}

% Enfin, on peut chercher à définir une fonction de décision dont la valeur
% absolue quantifie notre confiance en sa prédiction. On cherche alors à ce que
% $y f(\xx)$ soit la plus grande possible, et on utilise le {\it coût
%   logistique}.

\subsection{Coût logistique et entropie croisée}
\label{sec:logistic_loss}
On appelle {\it fonction de coût logistique}, ou {\it logistic loss}, la
fonction suivante :
\begin{align*}
  L_{\log} : \{-1, 1\} \times \RR & \rightarrow \RR \\
  y, f(\xx) & \mapsto \log \left( 1 + \exp(-y f(\xx))\right).
\end{align*}

Si l'on préfère utiliser $\YY = \{0, 1\}$, le coût logistique est équivalent à
l'{\it entropie croisée}.

\label{sec:cross_entropy}
Dans le cas binaire, on appelle {\it entropie croisée}, ou {\it cross-entropy},
la fonction suivante :
\begin{align*}
  L_H : \{0, 1\} \times ]0, 1[ & \rightarrow \RR \\
  y, f(\xx) & \mapsto - y \log f(\xx) - (1-y) \log(1-f(\xx)).
\end{align*}

L'entropie croisée est issue de la théorie de l'information, d'où son nom. En
considérant que la véritable classe de $\xx$ est modélisée par une distribution
$Q$, et sa classe prédite par une distribution $P$, nous allons chercher à
modéliser $P$ de sorte qu'elle soit la plus proche possible de $Q$. On utilise
pour cela la {\it divergence de Kullback-Leibler}:
\begin{align*}
  KL(Q||P) & = \sum_{c=0, 1} Q(y=c|\xx) \log \frac{Q(y=c|\xx}{P(y=c|\xx)} \\
           & = - \sum_{c=0, 1} Q(y=c|\xx) \log P(y=c|\xx) + 
             \sum_{c=0, 1} Q(y=c|\xx) \log Q(y=c|\xx)
\end{align*}
Comme $Q(y=c|\xx)$ vaut soit $0$ ($c$ n'est pas la classe de $\xx$) soit
$1$ (dans le cas contraire), le deuxième terme de cette expression est nul
et on retrouve ainsi la définition ci-dessus de l'entropie croisée.


% Les fonctions de perte pour la classification binaire sont illustrées sur la
% figure~\ref{fig:classif_losses}.
% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.7\textwidth]{figures/erm/classif_losses}
%   \caption{Fonctions de perte pour la classification binaire.}
%   \label{fig:classif_losses}
% \end{figure}

% \subsection{Coûts pour la régression}
% Dans le cas d'un problème de régression, nous considérons maintenant
% $\YY=\RR.$ Le but de notre fonction de coût est de pénaliser les fonctions de
% prédiction $f$ dont la valeur est éloignée de la valeur cible $\xx$.
\subsection{Coût quadratique pour la régression}
\label{sec:quadratic_loss}
On appelle {\it fonction de coût quadratique}, ou {\it quadratic loss}, ou
encore {\it squared error}, la fonction suivante :
\begin{align*}
  L_{\text{SE}} : \RR \times \RR & \rightarrow \RR \\
  y, f(\xx) & \mapsto \frac{1}{2} \left(y - f(\xx)\right)^2.
\end{align*}
Le coefficient $\frac{1}{2}$ permet d'éviter d'avoir des coefficients
multiplicateurs quand on dérive le risque empirique pour le minimiser.

% \subsubsection{Coût $\epsilon$-insensible}
% \label{sec:epsilon_insensitive}
% Le coût quadratique a tendance à être dominé par les valeurs aberrantes : dès
% que quelques observations dans le jeu de données ont une prédiction très
% éloignée de leur étiquette réelle, la qualité de la prédiction sur les autres
% observations importe peu. On peut ainsi lui préférer le {\it coût absolu} : \\

% On appelle {\it fonction de coût absolu}, ou {\it absolute error}, la fonction
% suivante :
% \begin{align*}
%   L_{\text{AE}} : \RR \times \RR & \rightarrow \RR \\
%   y, f(\xx) & \mapsto  |y - f(\xx)|.
% \end{align*}

% Avec cette fonction de coût, même les prédictions très proches de la véritable
% étiquette sont pénalisées (même si elles le sont faiblement). Cependant, il est
% numériquement quasiment impossible d'avoir une prédiction exacte. Le coût {\it
%   $\epsilon$-insensible} permet de remédier à cette limitation.

% Étant donné $\epsilon > 0$, on appelle {\it fonction de coût
%   $\epsilon$-insensible}, ou {\it $\epsilon$-insensitive loss}, la fonction
% suivante :
% \begin{align*}
%   L_\epsilon : \RR \times \RR & \rightarrow \RR \\
%   y, f(\xx) & \mapsto \max \left(0, |y - f(\xx)| - \epsilon\right).
% \end{align*}
    
% \subsubsection{Coût de Huber}
% Le coût $\epsilon$-insensible n'est dérivable ni en $-\epsilon$ ni en
% $+\epsilon$, ce qui complique l'optimisation du risque empirique.  La {\it
%   fonction de coût de Huber} permet d'établir un bon compromis entre le coût
% quadratique (dérivable en $0$) et le coût absolu (qui n'explose pas dans les
% valeurs extrêmes).

% On appelle {\it fonction de coût de Huber}, ou {\it Huber loss}, la fonction
% suivante :
% \begin{align*}
%   L_{\text{Huber}} : \RR \times \RR & \rightarrow \RR \\
%   y, f(\xx) & \mapsto
%               \begin{cases}
%                 \frac{1}{2} \left(y - f(\xx)\right)^2 & \text{ si } |y - f(\xx)| < \epsilon \\
%                 \epsilon |y - f(\xx)| - \frac{1}{2} \epsilon^2 & \text{ sinon.} 
%               \end{cases}
% \end{align*}
% Le terme $- \frac{1}{2} \epsilon^2$ permet d'assurer la continuité de la fonction.\\

% Les fonctions de coût pour la régression sont illustrées sur la
% figure~\ref{fig:regression_losses}.

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.7\textwidth]{figures/erm/regression_losses}
%   \caption{Fonctions de coût pour un problème de régression.}
%   \label{fig:regression_losses}
% \end{figure}



\section{Apprentissage supervisé d'un modèle paramétrique}
\subsection{Modèles paramétriques}
On parle de {\it modèle paramétrique} quand on utilise un algorithme
d'apprentissage dont le but est de trouver les valeurs optimales des paramètres
d'un modèle dont on a défini la forme analytique en fonction des descripteurs.

La complexité d'un modèle paramétrique grandit avec le nombre de paramètres à
apprendre, autrement dit avec le nombre de variables. À l'inverse, la
complexité d'un modèle non paramétrique aura tendance à grandir avec le nombre
d'observations.

Par exemple, un algorithme d'apprentissage qui permet d'apprendre les
coefficient $\alpha$, $\beta$, $\gamma$ dans la fonction de décision suivante :
$f: \xx \mapsto \alpha x_1 + \beta x_2x_4^2 + \gamma e^{x_3-x_5}$ apprend un
modèle paramétrique. Quel que soit le nombre d'observations, ce modèle ne
change pas.

À l'inverse, la méthode du plus proche voisin, qui associe à $\xx$ l'étiquette
du point du jeu d'entraînement dont il est le plus proche en distance
euclidienne, apprend un modèle non paramétrique : on ne sait pas écrire la
fonction de décision comme une fonction des variables prédictives. Plus il y a
d'observations, plus le modèle pourra apprendre une frontière de décision
complexe.

Étant donné un jeu $\DD = \{\xx^i, y^i\}_{i=1, \dots, n}$ de $n$ observations
en $p$ dimensions et leurs étiquettes réelles, nous supposons ici que la
fonction de décision $f$ est paramétrée par le vecteur
$\bbeta \in \mathbb{R}^{m}$.

Nous allons faire l'hypothèse que les erreurs, c'est-à-dire la différence entre
les étiquettes réelles et les valeurs correspondantes de $f$, sont normalement
distribuées, centrées en $0:$
\begin{equation}
  \label{eq:linreg_gaussian_error}
  y = f(\xx|\bbeta) + \epsilon \hspace{2em} \epsilon \sim \Ncal(0, \sigma^2).
\end{equation}
Cette hypothèse est illustrée sur la figure~\ref{fig:linreg} dans le cas
d'une fonction de décision linéaire.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/erm/linreg}
  \caption{Pour une observation $x^*$ donnée (ici en une dimension) , la
    distribution des valeurs possibles de l'étiquette $y^*$ correspondante est
    une gaussienne centrée en $f(x^*)$.}
  \label{fig:linreg}
\end{figure}

Selon cette hypothèse, les observations $\xx$ sont la réalisation de $p$
variables aléatoires $X_1, X_2, \dots, X_p$ à valeurs réelles, leurs étiquettes
$y$ sont la réalisation d'une variable aléatoire $Y$ à valeurs réelles, et ces
variables aléatoires vérifient
\begin{equation}
  \label{eq:linreg_bayes}
  \PP(Y=y|X = \xx) \sim 
  \Ncal\left(f(\xx|\bbeta), \sigma^2\right).
\end{equation}
On note ici $\PP(X=\xx)$ pour
$\PP(X_1=x_1, X_2=x_2, \dots, X_p=x_p).$

\subsection{Estimation par maximum de vraisemblance et méthode des
  moindres carrés}
\label{sec:least_squares}
Sous l'hypothèse~\ref{eq:linreg_bayes}, et en supposant les $n$ observations
indépendantes et identiquement distribuées, le log de vraisemblance du
paramètre $\bbeta$ vaut:
\begin{align*}
  \log \PP(\DD|\bbeta) & = \log \prod_{i=1}^n \PP(X = \xx^i|\bbeta) \\
                       & = \log \prod_{i=1}^n \PP(y^i|\xx^i) + \log \prod_{i=1}^n
                         \PP(X = \xx^i) \\
                       & = - \log \frac1{2\sigma^2} \sum_{i=1}^n \left(y^i -
                         f(\xx^i|\bbeta) \right)^2 + \Ccal
\end{align*}
Dans cette dernière équation, $\Ccal$ est une constante par rapport à $\bbeta$,
et provient d'une part du coefficient $\frac1{\sqrt{2\pi}}$ de la distribution
normale et d'autre part des $\PP(X=\xx^i).$

Ainsi, maximiser la vraisemblance revient à minimiser
$\sum_{i=1}^n \left(y^i - f(\xx^i|\bbeta) \right)^2$ : c'est ce que l'on
appelle la {\it minimisation des moindres carrés}, une méthode bien connue
depuis Gauss et Legendre. Notons aussi que cela revient à minimiser le risque
empirique quand il est défini en utilisant la fonction de coût
quadratique~\ref{sec:quadratic_loss}.

\section{Régression linéaire}
Commençons par considérer des modèles {\it linéaires}: nous cherchons à
expliquer la variable cible $y$ par une {\it combinaison linéaire} -- en
d'autres mots une somme pondérée -- des descripteurs.

\subsection{Formulation}
Nous choisissons une fonction de décision $f$ de la forme
\begin{equation}
  \label{eq:linear_decision}
  f: \xx \mapsto \beta_0 + \sum_{j=1}^p \beta_j x_j.
\end{equation}
Ici, $\bbeta \in \RR^{p+1}$ et donc $m=p+1$.

\subsection{Solution}
On appelle {\it régression linéaire} le modèle de la forme
$f: \xx \mapsto \beta_0 + \sum_{j=1}^p \beta_j x_j$ dont les coefficients sont
obtenus par minimisation de la somme des moindres carrés, à savoir :
\begin{equation}
  \label{eq:linreg}
  \argmin_{\bbeta \in \RR^{p+1}}  \sum_{i=1}^n \left(y^i - \left(\beta_0 + 
      \sum_{j=1}^p \beta_j x_j \right)\right)^2.
\end{equation}
  
Nous pouvons réécrire le problème~\ref{eq:linreg} sous forme matricielle, en
ajoutant à gauche à la matrice d'observations $X \in \RR^p$ une colonne de 1 :
\begin{equation}
  \label{eq:added_ones}
  X \leftarrow   \begin{pmatrix}
    1 & x_1^1 & \cdots & x_p^1 \\
    \vdots & \vdots & \cdots & \vdots \\
    1 & x_1^n& \cdots & x_p^n \\
  \end{pmatrix}.
\end{equation}

La somme des moindres carrés s'écrit alors
\begin{equation}
  \label{eq:rss_linreg}
  \text{RSS} = \left(\yy - X \bbeta\right)^\top \left(\yy -  X \bbeta\right).
\end{equation}

Il s'agit d'une forme quadratique convexe en $\bbeta$, que l'on peut donc
minimiser en annulant son gradient
$\nabla_{\bbeta} \text{RSS} = -2 X^\top \left(\yy - X \bbeta \right)$. On
obtient alors
\begin{equation}
  \label{eq:linreg_sol}
  X^\top X \bbeta^* = X^\top \yy.
\end{equation}
  
Si le rang de la matrice $X$ est égal à son nombre de colonnes, alors la
somme des moindres carrés~\ref{eq:rss_linreg} est minimisée pour
\begin{equation*}
  \bbeta^* = \left(X^\top X \right)^{-1} X^\top \yy.
\end{equation*}

Preuve :  Si $X$ est de rang colonne plein, alors $X^\top X$ est inversible.\\
  
Si $X^\top X$ n'est pas inversible, on pourra néanmoins trouver une solution
(non unique) pour $\bbeta$ en utilisant à la place de
$\left(X^\top X \right)^{-1}$ un pseudo-inverse (par exemple, celui de
Moore-Penrose) de $X^\top X$, c'est-à-dire une matrice $M$ telle que
$X^\top X M X^\top X = X^\top X.$

On peut aussi (et ce sera préférable quand $p$ est grand et que l'inversion de
la matrice $X^\top X \in \RR^{p \times p}$ est donc coûteuse) obtenir une
estimation de $\bbeta$ par un algorithme à directions de descente.

On fera attention à ne pas confondre les {\it variables}, qui sont les $p$
valeurs $x_1, x_2, \dots, x_p$ qui décrivent les données, et les {\it
  paramètres}, qui sont les $p+1$ valeurs $\beta_0, \beta_1, \dots, \beta_p$
qui paramètrent le modèle.

La régression linéaire produit un modèle interprétable, au sens où les
$\beta_j$ permettent de comprendre l'importance relative des variables sur la
prédiction. En effet, plus $\lvert \beta_j \rvert$ est grande, plus la $j$-ème
variable a un effet important sur la prédiction, et le signe de $\beta_j$ nous
indique la direction de cet effet.

Attention ! Cette interprétation n'est valide que si les variables ne sont pas
corrélées, et que $x_j$ peut être modifiée sans perturber les autres
variables. De plus, si les variables sont corrélées, $X$ n'est pas de rang
colonne plein et $X^\top X$ n'est donc pas inversible. Ainsi la régression
linéaire admet plusieurs solutions. Intuitivement, on peut passer de l'une à
l'autre de ces solutions car une perturbation d'un des poids $\beta_j$ peut
être compensée en modifiant les poids des variables corrélées à $x_j$.





  
% \section{Points clés}
% \begin{itemize}
% \item Les trois ingrédients d'un algorithme d'apprentissage supervisé sont :
%   \begin{itemize}
%   \item l'espace des hypothèses,
%   \item la fonction de coût,
%   \item l'algorithme d'optimisation qui permet de trouver l'hypothèse
%     optimale au sens de la fonction de coût sur les données (minimisation du
%     risque empirique).
%   \end{itemize}
% \item On peut apprendre les coefficients d'un modèle de régression paramétrique
%   par maximisation de vraisemblance, ce qui équivaut à minimiser le risque
%   empirique en utilisant le coût quadratique comme fonction de perte, et
%   revient à la méthode des moindres carrés.
% \item La régression linéaire admet une unique solution $\bbeta^* = \left(X^\top
%     X \right)^{-1} X^\top \yy$ si et seulement si $X^\top X$ est
%   inversible. Dans le cas contraire, il existe une infinité de solutions.

% \end{itemize}


% % \begin{plusloin}
% % \item La notion de complexité d'un modèle a été formalisée par Vladimir Vapnik et
% %   Alexey Chervonenkis dans les années 1970, et est détaillée par exemple
% %   dans l'ouvrage de \citet{vapnik1995}.
% % \item Pour en savoir plus sur la théorie de l'apprentissage, on pourra se
% %   référer au livre de \citet{kearns1994}.
% % \item On trouvera une discussion détaillée du compromis biais-variance dans
% %   \citet{friedman1997}.
% % \end{plusloin}

% % \section*{Bibliographie}
% % \vspace{-25pt}
% % \begin{thebibliography}{99}
% % \bibitem[\protect\astroncite{Crammer and Singer}{2001}]{crammer2001}
% % Crammer, K. and Singer, Y. (2001).
% % \newblock On the algorithmic implementation of multiclass kernel-based vector
% %   machines.
% % \newblock {\em Journal of Machine Learning Research}, 2:265--292.

% % \bibitem[\protect\astroncite{Friedman}{1997}]{friedman1997} Friedman,
% %   J.~H. (1997).  \newblock On bias, variance, 0/1-loss and the curse of
% %   dimensionality.  \newblock {\em Data Mining and Knowledge Discovery},
% %   1:55--77.

% % \bibitem[\protect\astroncite{Kearns et Vazirani}{1994}]{kearns1994} Kearns,
% %   M.~J. et Vazirani, U.~V. (1994).  \newblock {\em An Introduction to
% %     Computational Learning Theory}.  \newblock MIT Press, Cambridge, MA.

% % \bibitem[\protect\astroncite{Vapnik}{1995}]{vapnik1995} Vapnik, V.~N. (1995).
% %   \newblock {\em The Nature of Statistical Learning Theory}.  \newblock
% %   Springer, New York.

% % \bibitem[\protect\astroncite{Weston and Watkins}{1999}]{weston1999}
% % Weston, J. and Watkins, C. (1999).
% % \newblock Support vector machines for multi-class pattern recognition.
% % \newblock In {\em European Symposium on Artificial Neural Networks}.
% % \end{thebibliography}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "sdd_2020_poly"
%%% End: