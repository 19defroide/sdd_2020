%-*- coding: iso-latin-1 -*-
\label{chap:dimred}

\todo{
  \begin{itemize}
  \item Cohérence des notations avec les chapitres précédents.
  \item Lien avec le cours d'optimisation.
  \item Alléger / élaguer.
  \end{itemize}
}


\paragraph{Notions :} sélection de variables ; extraction de variables ;
filtrage ; analyse en composantes principales ; analyse en composantes principales probabiliste.
\paragraph{Objectifs pédagogiques :} 
\begin{itemize}      
  \setlength{\itemsep}{3pt}
\item Expliquer l'intérêt de réduire la dimension d'un jeu de données ;
\item Faire la différence entre la sélection de variables et l'extraction de variables ;
\item Mettre en \oe{}uvre des méthodes de sélection de variable par filtrage ;
\item Projeter des données sur un espace de plus petite dimension ;%par factorisation de matrice ; 
\item Mettre en \oe{}uvre des méthodes d'extraction de variables.
\end{itemize}

\section{Notations}
Nous avons jusqu'à présent travaillé sur des séries statistiques contenant une
seule variable. Cependant, dans la majorité des problèmes de sciences des
données, nous disposons de plusieurs variables pour décrire chaque individu.

À partir de maintenant, l'objet de nos études ne sera plus un échantillon
$(x_1, x_2, \dots, x_n),$ mais une matrice $X \in \RR^{n \times p}$
représentant $n$ individus, ou observations, décrites chacune par $p$
variables.

Nous essaierons de nous en tenir aux notations suivantes :
\begin{itemize}
\item Les lettres minuscules ($x$) représentent un scalaire ;
\item les lettres minuscules surmontées d'une flèche ($\xx$) représentent un
  vecteur ;
\item les lettres majuscules ($X$) représentent une matrice, un événement ou
  une variable aléatoire ;
\item les lettres calligraphiées ($\XX$) représentent un ensemble ou un espace ;
\item les {\it indices} correspondent à une variable tandis que les {\it
    exposants} correspondent à une observation : $x^i_j$ est la $j$-ème
  variable de la $i$-ème observation, et correspond à l'entrée $X_{ij}$ de la
  matrice $X$ ;
\item $n$ est un nombre d'observations et $p$ un nombre de variables.
\end{itemize}

\section{Motivation}
Le but de la réduction de dimension est de transformer une représentation
$X \in \RR^{n \times p}$ des données en une représentation
$X^* \in \RR^{n \times m}$ où $m \ll p$. Les raisons de cette démarche sont
multiples : 

\begin{itemize}
\item \textbf{Visualiser les données :} ce n'est pas tâche aisée avec un nombre
  très grand de variables. Comment visualiser $n$ points en plus de 2 ou 3
  dimensions ? Limiter les variables à un faible nombre de dimensions permet de
  visualiser les données plus facilement, quitte à perdre un peu d'information
  lors de la transformation.
\item \textbf{Réduire les coûts algorithmiques du traitement des données :} d'un
  point de vue purement computationnel, réduire la dimension des données permet
  de réduire d'une part l'espace qu'elles prennent en mémoire et d'autre part
  les temps de calcul. De plus, si certaines variables sont inutiles, ou
  redondantes, il n'est pas nécessaire de les obtenir pour de nouvelles
  observations : cela permet de réduire le coût d'acquisition des données.
\item \textbf{Améliorer la qualité du traitement des données :} les algorithmes
  d'apprentissage supervisé ou de clustering sont généralement plus performants
  sur un faible nombre de variables. Tout d'abord, si certaines des variables
  ne sont pas pertinentes, elles risquent d'induire du biais dans les modèles
  appris. De plus, les raisonnements développés en faible dimension pour
  construire un algorithme d'apprentissage supervisé ne s'appliquent pas
  nécessairement en haute dimension. C'est un phénomène connu sous le nom de
  {\it fléau de la dimension}, ou {\it curse of dimensionality} en
  anglais. Pour plus de détails, reportez-vous à la section~\ref{sec:dimcurse}.
\end{itemize}


Deux possibilités s'offrent à nous pour réduire la dimension de nos données :
\begin{itemize}
\item la \textbf{sélection de variables}, qui consiste à éliminer un nombre
  $p-m$ de variables de nos données ;
\item l'\textbf{extraction de variables}, qui consiste à {\it créer} $m$
  nouvelles variables à partir des $p$ variables dont nous disposons
  initialement.
\end{itemize}

\paragraph{Sélection de variables} La sélection de variables consiste à
éliminer des variables peu informatives. Dans le cas non-supervisé, il s'agit
par exemple d'éliminer des variables dont la variance est très faible (leur
valeur étant à peu près la même pour chaque individu, elle n'apporte aucune
information permettant de distinguer deux individus) ou qui sont corrélées à
une autre variable (elles apportent alors la même information).

Dans le cas supervisé, il s'agit aussi d'éliminer des variables qui ne sont pas
pertinentes par rapport à la tâche de prédiction. On peut par exemple
\begin{itemize}
\item éliminer, par exemple à l'aide d'un test du chi2 comme vu dans la PC1,
  les variables indépendantes de l'étiquette à prédire. Remarquez néanmoins que
  deux variables chacune indépendante de l'étiquette peuvent être très
  informatives quand on les considère simultanément\footnote{Considérez par exemple, en deux dimensions, un problème de classification dans lequel l'étiquette $y$ est donnée par $y = x_1 \oplus x_2$} ;
\item chercher à éliminer des variables qui n'améliorent pas la performance
  d'un algorithme précis.
\end{itemize}
Nous reviendrons sur la
sélection de variables supervisée quand nous parlerons du lasso
(section~\ref{sec:lasso}).


%La suite de ce chapitre détaille des exemples de ces deux approches.

\section{Analyse en composantes principales}
\label{sec:pca}
La méthode la plus classique pour réduire la dimension d'un jeu de données par
extraction de variables est l'\textbf{analyse en composantes principales}, ou
{\it ACP}. On parle aussi souvent de {\it PCA}, de son nom anglais {\it
  Principal Component Analysis}.

\subsection{Maximisation de la variance}
L'idée centrale d'une ACP est de représenter les données de sorte à maximiser
leur variance selon les nouvelles dimensions, afin de pouvoir continuer à
distinguer les exemples les uns des autres dans leur nouvelle représentation
(cf. figure~\ref{fig:data_variance}).
\begin{figure}[h]
  \centering
  \includegraphics[width=0.4\textwidth]{figures/dimred/data_variance}
  \caption{La variance des données en deux dimensions est maximale selon l'axe
    indiqué par la flèche.}
  \label{fig:data_variance}
\end{figure}

Formellement, une nouvelle représentation de $\XX$ est définie par une base
orthonormée sur laquelle projeter la matrice de données $X$.

Une \textbf{analyse en composantes principales}, ou {\it ACP}, de la matrice
$X \in \RR^{n \times p}$ est une transformation linéaire orthogonale qui permet
d'exprimer $X$ dans une nouvelle base orthonormée, de sorte que la plus grande
variance de $X$ par projection s'aligne sur le premier axe de cette nouvelle
base, la seconde plus grande variance sur le deuxième axe, et ainsi de suite.

Les axes de cette nouvelle base sont appelés les \textbf{composantes
  principales}, abrégées en {PC} pour {\it Principal Components}.

\begin{attention}
  Dans la suite de cette section, nous supposons que les variables ont été {\it
    standardisées} de sorte à toutes avoir une moyenne de 0 et une variance de
  1, pour éviter que les variables qui prennent de grandes valeurs aient plus
  d'importance que celles qui prennent de faibles valeurs. C'est un pré-requis
  de l'application de l'ACP.  Cette standardisation s'effectue en centrant la
  moyenne et en réduisant la variance de chaque variable :
  \begin{equation}
    \label{eq:standardization}
    x^i_j \leftarrow \frac{x^i_j - \bar{x_j}}{\sqrt{\frac1n 
        \sum_{l=1}^n (x^l_j - \bar{x_j})^2}},
  \end{equation}
  où $\bar{x_j} = \frac1n \sum_{l=1}^n x^l_j.$ On dira alors que $X$ est {\it
    centrée} : chacune de ses colonnes a pour moyenne 0.
\end{attention}

\paragraph{Théorème} 
Soit $X \in \RR^{n \times p}$ une matrice {\it centrée} de covariance
$\Sigma = \frac1n X^\top X$. Les composantes principales de $X$ sont les
vecteurs propres de $\Sigma$, ordonnés par valeur propre décroissante.

\paragraph{Preuve}
Commençons par démontrer que, pour tout vecteur $\ww \in \RR^p$, la variance de
la projection de $X$ sur $\ww$ vaut $w^\top \Sigma w.$

La projection de $X \in \RR^{n \times p}$ sur $\ww \in \RR^p$ est le vecteur
$\zz = X w.$ Comme $X$ est centrée, la moyenne de $\zz$ vaut
\begin{equation*}
  \frac1n \sum_{i=1}^n z_i = \frac1n \sum_{i=1}^n \sum_{j=1}^p x^i_j w_j = 
  \frac1n \sum_{j=1}^p  w_j \sum_{i=1}^n x^i_j = 0.
\end{equation*}
Sa variance vaut
\begin{equation*}
  \text{Var}[\zz] = \frac1n \sum_{i=1}^n zz_i^2 = \frac1n \ww^\top X^\top X \ww
  = \ww^\top \Sigma \ww.
\end{equation*}    

Appelons maintenant $\ww_1 \in \RR^p$ la première composante
principale. $\ww_1$ est orthonormé et tel que la variance de $X \ww_1$ soit
maximale :
\begin{equation}
  \label{eq:pc1}
  \ww_1 = \argmax_{\ww \in \RR^p} \ww^\top \Sigma \ww \text{ avec } \ltwonorm{\ww_1}=1.
\end{equation}
Il s'agit d'un problème d'optimisation quadratique sous contrainte d'égalité,
que l'on peut résoudre en introduisant le multiplicateur de Lagrange
$\alpha_1 > 0$ et en écrivant le lagrangien
\begin{equation*}
  L(\alpha_1, \ww) = \ww^\top \Sigma \ww - \alpha_1 
  \left( \ltwonorm{\ww} - 1 \right).
\end{equation*}
Par dualité forte, % $\min_{\ww \in \RR^p} w^\top \Sigma w = \max_{\alpha_1}
    % \inf_{\ww \in \RR^p} L(\alpha_1, \ww).$
le maximum de $\ww^\top \Sigma \ww$ sous la contrainte $\ltwonorm{\ww_1}=1$ est
égal à $\min_{\alpha_1} \sup_{\ww \in \RR^p} L(\alpha_1, \ww).$ Le supremum du
lagrangien est atteint en un point où son gradient s'annule, c'est-à-dire qui
vérifie
\begin{equation*}
  2 \Sigma \ww - 2 \alpha_1 \ww = 0.
\end{equation*}
Ainsi, $\Sigma \ww_1 = \alpha_1 \ww_1$ et $(\alpha_1, \ww_1)$ forment un couple
(valeur propre, vecteur propre) de $\Sigma$.

Parmi tous les vecteurs propres de $\Sigma$, $\ww_1$ est celui qui maximise la
variance $\ww_1^\top \Sigma \ww_1 = \alpha_1 \ltwonorm{\ww_1} = \alpha_1.$
Ainsi, $\alpha_1$ est la plus grande valeur propre de $\Sigma$ (rappelons que
$\Sigma$ étant définie par $X^\top X$ est semi-définie positive et que toutes
ses valeurs propres sont positives.)

La deuxième composante principale de $X$ vérifie
\begin{equation}
  \label{eq:pc2}
  \ww_2 = \argmax_{\ww \in \RR^p} \ww^\top \Sigma \ww \text{ avec } \ltwonorm{\ww_2}=1
  \text{ et } \ww^\top\ww_1=0.
\end{equation}
Cette dernière contrainte nous permet de garantir que la base des composantes
principales est orthonormée.

Nous introduisons donc maintenant deux multiplicateurs de Lagrange
$\alpha_2 > 0$ et $\beta_2 > 0$ et obtenons le lagrangien
\begin{equation*}
  L(\alpha_2, \beta_2, \ww) = \ww^\top \Sigma \ww - \alpha_2 
  \left(\ltwonorm{\ww}^2 - 1 \right)
  - \beta_2 \ww^\top\ww_1.
\end{equation*}
Comme précédemment, son supremum en $\ww$ est atteint en un point où son
gradient s'annule :
\begin{equation*}
  2 \Sigma \ww_2 - 2 \alpha_2 \ww_2 - \beta_2 \ww_1 = 0.
\end{equation*}
En multipliant à gauche par $\ww_1^\top$, on obtient
\begin{equation*}
  2 \ww_1^\top \Sigma \ww_2 - 2 \alpha_2 \ww_1^\top \ww_2 - 
  \beta_2 \ww_1^\top \ww_1 = 0
\end{equation*}
d'où l'on conclut que $\beta_2=0$ et, en remplaçant dans l'équation précédente,
que, comme pour $\ww_1$, $2 \Sigma \ww_2 - 2 \alpha_1 \ww_2 = 0$.  Ainsi
$(\alpha_2, \ww_2)$ forment un couple (valeur propre, vecteur propre) de
$\Sigma$ et $\alpha_2$ est maximale : il s'agit donc nécessairement de la
deuxième valeur propre de $\Sigma$.
    
Le raisonnement se poursuit de la même manière pour les composantes principales
suivantes. \hfill $\square$

  
\paragraph{Remarque} Alternativement, on peut prouver ce théorème
en observant que $\Sigma$, qui est par construction définie positive, est
diagonalisable par un changement de base orthonormée :
$\Sigma = Q^\top \Lambda Q$, où $\Lambda \in \mathbb{R}^{p \times p}$ est une
matrice diagonale dont les valeurs diagonales sont les valeurs propres de
$\Sigma$.  Ainsi,
\begin{equation*}
  \ww_1^\top \Sigma \ww_1 = \ww_1^\top Q^\top \Lambda Q \ww_1 = 
  \left(Q \ww_1 \right)^\top \Lambda \left(Q \ww_1 \right).
\end{equation*}
Si l'on pose $\vv = Q \ww_1$, il s'agit donc de trouver $\vv$ de norme 1 ($Q$
étant orthonormée et $\ww_1$ de norme 1) qui maximise
$\sum_{j=1}^p v_j^2 \lambda_j$.  Comme $\Sigma$ est définie positive,
$\lambda_j \geq 0 \; \forall j=1, \dots, p$. De plus, $\ltwonorm{\vv} = 1$
implique que $0 \leq v_j^1 \leq 1 \; \forall j=1, \dots, p.$ Ainsi,
$\sum_{j=1}^p v_j^2 \lambda_j \leq \max_{j=1, \dots, p} \lambda_j \sum_{j=1}^p
v_j^2 \leq \max_{j=1, \dots, p} \lambda_j$
et ce maximum est atteint quand $v_j=1$ et $v_k=0 \; \forall k \neq j$. On
retrouve ainsi que $\ww_1$ est le vecteur propre correspondant à la plus grande
valeur propre de $\Sigma$, et ainsi de suite.


\subsection{Décomposition en valeurs singulières}
\paragraph{Théorème} 
Soit $X \in \RR^{n \times p}$ une matrice {\it centrée}. Les composantes
principales de $X$ sont ses vecteurs singuliers à droite ordonnés par valeur
singulière décroissante.

\paragraph{Preuve}
Si l'on écrit $X$ sous la forme $U D V^\top$ où $U \in \RR^{n \times n}$ et
$V \in \RR^{p \times p}$ sont orthogonales, et $D \in \RR^{n \times p}$ est
diagonale, alors
\begin{equation*}
  \Sigma = X^\top X = V D U^\top U D V^\top = V D^2 V^\top
\end{equation*}
et les valeurs singulières de $X$ (les entrées de $D$) sont les racines carrées
des valeurs propres de $\Sigma$, tandis que les vecteurs singuliers à droite de
$X$ (les colonnes de $V$) sont les vecteurs propres de $\Sigma$. \hfill $\square$

  
En pratique, les implémentations de la décomposition en valeurs singulières (ou
SVD) sont numériquement plus stables que celles de décomposition spectrale. On
préfèrera donc calculer les composantes principales de $X$ en calculant la SVD
de $X$ plutôt que la décomposition spectrale de $X^\top X$.

\subsection{Choix du nombre de composantes principales}
Réduire la dimension des données par une ACP implique de {\it choisir} un
nombre de composantes principales à conserver. Pour ce faire, on utilise la
\textbf{proportion de variance expliquée} par ces composantes : la variance de $X$
s'exprime comme la trace de $\Sigma$, qui est elle-même la somme de ses valeurs
propres.

Ainsi, si l'on décide de conserver les $m$ premières composantes principales de
$X$, la proportion de variance qu'elles expliquent est
\begin{equation*}
  \frac{\alpha_1 + \alpha_2 + \dots + \alpha_m}{\text{Tr}(\Sigma)}
\end{equation*}
où $\alpha_1 \geq \alpha_2 \geq \dots \geq \alpha_p$ sont les valeurs propres
de $\Sigma$ par ordre décroissant.
  
Il est classique de s'intéresser à l'évolution, avec le nombre de composantes,
soit de la proportion de variance expliquée par chacune d'entre elles, soit à
cette proportion cumulée, que l'on peut représenter visuellement sur un {\it
  scree plot} (figure~\ref{fig:scree_plot}). Ce graphe peut nous servir à
déterminer :
\begin{itemize}
\item soit le nombre de composantes principales qui explique un pourcentage de
  la variance que l'on s'est initialement fixé (par exemple, sur la
  figure~\ref{fig:scree_plot_cumul}, $95\%$) ;
\item soit le nombre de composantes principales correspondant au « coude » du
  graphe, à partir duquel ajouter une nouvelle composante principale ne semble
  plus informatif.
\end{itemize}

\begin{figure}[h]
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/dimred/scree_plot}
    \caption{Pourcentage de variance expliqué par chacune des composantes
      principales. À partir de 6 composantes principales, ajouter de nouvelles
      composantes n'est plus vraiment informatif.}
    \label{fig:scree_plot}
  \end{subfigure} \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \includegraphics[width=\textwidth]{figures/dimred/scree_plot_cumul}  
    \caption{Pourcentage cumulé de variance expliquée par chacune des
      composantes principales. Si on se fixe une proportion de variance
      expliquée de $95\%$, on peut se contenter de 10 composantes principales.}
    \label{fig:scree_plot_cumul}
  \end{subfigure}
  \caption{Pour choisir le nombre de composantes principales, on utilise le
    pourcentage de variance expliquée.}
\end{figure}

\section{Factorisation de la matrice des données}
Soit un nombre $m$ de composantes principales, calculées par une ACP,
représentées par une matrice $W \in \RR^{p \times m}$. La représentation
réduite des $n$ observations dans le nouvel espace de dimension $m$ s'obtient
en projetant $X$ sur les colonnes de $W$, autrement dit en calculant
\begin{equation}
  \label{eq:reduced_rep}
  H = W^\top X.
\end{equation}

La matrice $H \in \RR^{m \times n}$ peut être interprétée comme une
\textbf{représentation latente} (ou cachée, {\it hidden} en anglais d'où la
notation $H$) des données. C'est cette représentation que l'on a cherché à
découvrir grâce à l'ACP.

Les colonnes de $W$ étant des vecteurs orthonormés (il s'agit de vecteurs
propres de $X X^\top$), on peut multiplier l'équation~\ref{eq:reduced_rep} à
gauche par $W$ pour obtenir une {\it factorisation} de $X$ :
\begin{equation}
  \label{eq:pca_factor}
  X = W H.
\end{equation}
On appelle ainsi les lignes de $H$ les \textbf{facteurs} de $X$.


Cette factorisation s'inscrit dans le cadre plus général de \textbf{l'analyse
  factorielle}.

Supposons que les observations $\{\xx^1, \xx^2, \dots, \xx^n\}$ soient les
réalisations d'une variable aléatoire $p$-dimensionnelle $\xx$, générée par le
modèle
\begin{equation}
  \label{eq:fa_model}
  \xx = W \hh + \epsilon,
\end{equation}
où $\hh$ est une variable aléatoire $m$-dimensionnelle qui est la
représentation latente de $\xx$, et $\epsilon$ un bruit gaussien :
$\epsilon \sim \Ncal(0, \Psi).$

Supposons les données centrées en $0$, et les variables latentes
$\hh^1, \hh^2, \dots, \hh^n$ (qui sont des réalisations de $\hh$) indépendantes
et gaussiennes de variance unitaire, c'est-à-dire $\hh \sim \Ncal(0, I_m)$ où
$I_m$ est la matrice identité de dimensions $m \times m$. Alors $W \hh$ est
centrée en $0$, et sa covariance est $WW^\top.$ Alors
$\xx \sim \Ncal(0, WW^\top + \Psi)$.

Si on considère de plus que $\epsilon$ est {\it isotropique}, autrement dit que
$\Psi = \sigma^2 I_p$, alors $\xx \sim \Ncal(0, WW^\top + \sigma^2 I_p)$, on
obtient ce que l'on appelle \textbf{ACP probabiliste}. Les
paramètres $W$ et $\sigma$ de ce modèle peuvent être obtenus par maximum de
vraisemblance.

L'ACP classique est un cas limite de l'ACP probabiliste, obtenu quand la
covariance du bruit devient infiniment petite ($\sigma^2 \rightarrow 0$).

Plus généralement, on peut faire l'hypothèse que les variables observées
$x_1, x_2, \dots, x_p$ sont conditionnellement indépendantes étant données les
variables latentes $h_1, h_2, \dots, h_m.$ Dans ce cas, $\Psi$ est une matrice
diagonale, $\Psi = \text{diag}(\psi_1, \psi_2, \dots, \psi_p)$, où $\psi_j$
décrit la variance spécifique à la variable $x_j$. Les valeurs de $W$, $\sigma$
et $\psi_1, \psi_2, \dots, \psi_p$ peuvent encore une fois être obtenues par
maximum de vraisemblance. C'est ce que l'on appelle \textbf{l'analyse
  factorielle}.

Dans l'analyse factorielle, nous ne faisons plus l'hypothèse que les nouvelles
dimensions sont orthogonales. En particulier, il est donc possible d'obtenir
des dimensions dégénérées, autrement dit des colonnes de $W$ dont toutes les
coordonnées sont $0$.

% \subsection{Approches non linéaires}
% De nombreuses autres approches ont été proposées pour réduire la dimension des
% données de manière non linéaire. Parmi elles, nous en abordons ici
% quelques-unes parmi les plus populaires ; les expliquer de manière détaillée
% dépasse le propos de cet ouvrage introductif.

% \subsubsection{Analyse en composantes principales à noyau}
% Nous commencerons par noter que l'analyse en composantes principales se prête à
% l'utilisation de l'astuce du noyau (cf. section~\ref{sec:kernel_trick}). La
% méthode qui en résulte est appelée {\it kernel PCA}, ou {\it kPCA}.

% \subsubsection{Positionnement multidimensionnel}
% Le {\it positionnement multidimensionnel}, ou {\it multidimensional scaling}
% ({\it MDS})~\citep{cox1994}, se base sur une matrice de {\it dissimilarité}
% $D \in \RR^{n \times n}$ entre les observations : il peut s'agir d'une distance
% métrique, mais ce n'est pas nécessaire. Le but de l'algorithme est alors de
% trouver une représentation des données qui préserve cette dissimilarité :
% \begin{equation}
%   \label{eq:mds}
%   X^* = \argmin_{Z \in \RR^{n \times m}} \sum_{i=1}^n \sum_{l=i+1}^n \left( 
%     \ltwonorm{\zz^i - \zz^l} - D_{il}\right)^2.
% \end{equation}
% Si l'on utilise la distance euclidienne comme dissimilarité, alors le MDS est
% équivalent à une ACP.

% Le positionnement multidimensionnel peut aussi s'utiliser pour positionner dans
% un espace de dimension $m$ des points dont on ne connaît pas les
% coordonnées. Il s'applique par exemple très bien à repositionner des villes sur
% une carte à partir uniquement des distances entre ces villes.

% Une des limitations de MDS est de ne chercher à conserver la distance entre les
% observations que globalement. Une façon efficace de construire la matrice de
% dissimilarité de MDS de sorte à conserver la structure locale des données est
% l'algorithme {\it IsoMap}~\citep{tenenbaum2000}. Il s'agit de construire un
% graphe de voisinage entre les observations en reliant chacune d'entre elles à
% ses $k$ plus proches observations voisines. Ces arêtes peuvent être pondérée
% par la distance entre les observations qu'elles relient. Une dissimilarité
% entre observations peut ensuite être calculée sur ce graphe de voisinage, par
% exemple via la longueur du plus court chemin entre deux points.

% \subsubsection{t-SNE}
% Enfin, l'algorithme {\it t-SNE}, pour {\it t-Student Neighborhood Embedding},
% proposé en 2008 par Laurens van der Maaten and Geoff Hinton, propose
% d'approcher la distribution des distances entre observations par une loi de
% Student~\citep{vandermaaten2008}. Pour chaque observation $\xx^i$, on définit
% $P_i$ comme la loi de probabilité définie par
% \begin{equation}
%   P_i(\xx) = \frac1{\sqrt{2 \pi \sigma^2}} \exp \left( - \frac{
%       \ltwonorm{\xx - \xx^i}^2}{2 \sigma^2} \right).
%   \label{eq:tsne_distr}    
% \end{equation}
% t-SNE consiste alors à résoudre
% \begin{equation}
%   \label{eq:tsne}
%   \argmin_{Q} \sum_{i=1}^n KL(P_i||Q_i)
% \end{equation}
% où $KL$ dénote la divergence de Kullback-Leibler (voir
% section~\ref{sec:cross_entropy}) et $Q$ est choisie parmi les distributions de
% Student de dimension inférieure à $p$. Attention, cet algorithme trouve un
% minimum local et non global, et on pourra donc obtenir des résultats différents
% en fonction de son initialisation. De plus, sa complexité est quadratique en le
% nombre d'observations.
% \end{cours}

% \begin{pointsclefs}
% \item Réduire la dimension des données avant d'utiliser un algorithme
%   d'apprentissage supervisé permet d'améliorer ses besoins en temps et en
%   espace, mais aussi ses performances.
% \item On distingue la sélection de variables, qui consiste à éliminer des
%   variables redondantes ou peu informatives, de l'extraction de variable, qui
%   consiste à générer une nouvelle représentation des données.
% \item Projeter les données sur un espace de dimension 2 grâce à, par exemple,
%   une ACP ou t-SNE, permet de les visualiser.
% \item De nombreuses méthodes permettent de réduire la dimension des variables. 
% \end{pointsclefs}

\section{Compléments}
\subsection{Fléau de la dimension}
\label{sec:dimcurse}
En haute dimension, les individus ont tendance à tous être éloignés les uns des
autres. Pour comprendre cette assertion, plaçons-nous en dimension $p$ et
considérons l'hypersphère $\Scal(\xx, R)$ de rayon $R \in \RR_+^*$ centrée sur
une observation $\xx$, ainsi que l'hypercube $\Ccal(\xx, R)$ circonscrit à
cette hypersphère. Le volume de $\Scal(\xx)$ vaut
$\frac{2 R^p \pi^{p/2}}{p \Gamma(p/2)}$, tandis que celui de $\Ccal(\xx, R)$,
dont le côté a pour longueur $2R$, vaut $2^p R^p$. Ainsi
\begin{equation*}
  \lim_{p \rightarrow \infty} \frac{\text{Vol}(\Ccal(\xx, R))}{
    \text{Vol}(\Scal(\xx, R))} = 0.
\end{equation*}
Cela signifie que la probabilité qu'un exemple situé dans $\Ccal(\xx, R)$
appartienne à $\Scal(\xx, R)$, qui vaut $\frac{\pi}4 \approx 0.79$ lorsque
$p=2$ et $\frac{\pi}6 \approx 0.52$ lorsque $p=3$, devient très faible quand
$p$ est grand : les données ont tendance à être éloignées les unes des autres.

Cela implique que les algorithmes développés en utilisant une notion de
similarité ou distance entre individus ne fonctionnent pas nécessairement en
grande dimension. Ainsi, réduire la dimension peut être nécessaire à la
construction de bons modèles d'apprentissage.


\begin{plusloin}
\item \todo{Plus de variantes de l'analyse factorielle, e.g. NMF}
\item \todo{Representation learning}
\item \todo{MDS, IsoMap, tSNE}
% \item Le tutoriel de \citet{shlens2014} est une introduction détaillée à l'analyse en
%   composantes principales.
% \item Pour une revue des méthodes de sélection de variables, on pourra se
%   réferer à~\citet{guyon2003}.
% \item Pour plus de détails sur la NMF, on pourra par exemple se tourner
%   vers~\citet{lee1999}
% \item Pour plus de détails sur les méthodes des sélection de sous-ensemble de
%   variables (wrapper methods), on pourra se référer à
%   l'ouvrage de~\citet{miller1990}
% \item Une page web est dédiée à Isomap:
%   \url{http://web.mit.edu/cocosci/isomap/isomap.html}.
% \item Pour plus de détails sur l'utilisation de t-SNE, on pourra se référer à
%   la page \url{https://lvdmaaten.github.io/tsne/} ou à la publication
%   interactive de~\citet{wattenberg2016}.
\end{plusloin}

% \section*{Bibliographie}
% \vspace{-25pt}
% \begin{thebibliography}{99}
% \bibitem[\protect\astroncite{Cox et Cox}{1994}]{cox1994} Cox, T.~F. et Cox,
%   M. A.~A. (1994).  \newblock {\em Multidimensional Scaling}.  \newblock
%   Chapman and Hall., London.

% \bibitem[\protect\astroncite{Guyon et Elisseeff}{2003}]{guyon2003} Guyon,
%   I. et Elisseeff, A. (2003).  \newblock An introduction to variable and
%   feature selection.  \newblock {\em Journal of Machine Learning Research},
%   3:1157--1182.

% \bibitem[\protect\astroncite{Hinton}{2002}]{hinton2002} Hinton, G.~E. (2002).
%   \newblock Training product of experts by minimizing contrastive divergence.
%   \newblock {\em Neural Computation}, 14:1771--1800.

% \bibitem[\protect\astroncite{Hinton et Salakhutdinov}{2006}]{hinton2006}
%   Hinton, G.~E. et Salakhutdinov, R.~R. (2006).  \newblock Reducing the
%   dimensionality of data with neural networks.  \newblock {\em Science},
%   313:504--507.

% \bibitem[\protect\astroncite{Kozachenko et Leonenko}{1987}]{kozachenko1987}
%   Kozachenko, L.~F. et Leonenko, N.~N. (1987).  \newblock A statistical
%   estimate for the entropy of a random vector.  \newblock {\em Problemy
%     Peredachi Informatsii}, 23:9--16.

% \bibitem[\protect\astroncite{Lee et Seung}{1999}]{lee1999} Lee, D.~D. et
%   Seung, H.~S. (1999).  \newblock Learning the parts of objects by non-negative
%   matrix factorization.  \newblock {\em Nature}, 401(6755):788--791.

% \bibitem[\protect\astroncite{Miller}{1990}]{miller1990} Miller, A.~J. (1990).
%   \newblock {\em Subset Selection in Regression}.  \newblock Chapman and Hall.,
%   London.

% \bibitem[\protect\astroncite{Shlens}{2014}]{shlens2014} Shlens, J. (2014).
%   \newblock A {Tutorial} on {Principal} {Component} {Analysis}.  \newblock {\em
%     arXiv [cs, stat]}.  \newblock arXiv: 1404.1100.

% \bibitem[\protect\astroncite{Smolensky}{1986}]{smolensky1986} Smolensky,
%   P. (1986).  \newblock Information processing in dynamical systems:
%   foundations of harmony theory.  \newblock In {\em Parallel Distributed
%     Processing: Explorations in the Microstructure of Cognition}, volume 1:
%   Foundations, chapter~6, pages 194--281. MIT Press, Cambridge, MA.

% \bibitem[\protect\astroncite{Tenenbaum et~al.}{2000}]{tenenbaum2000} Tenenbaum,
%   J.~B., de~Silva, V., et Langford, J.~C. (2000).  \newblock A global
%   geometric framework for nonlinear dimensionality reduction.  \newblock {\em
%     Science}, 290(5500):2319--2323.

% \bibitem[\protect\astroncite{Tipping et Bishop}{1999}]{tipping1999} Tipping,
%   M.~E. et Bishop, C.~M. (1999).  \newblock Probabilistic principal components
%   analysis.  \newblock {\em Journal of the Royal Statistical Society Series B},
%   61:611--622.

% \bibitem[\protect\astroncite{van~der Maaten et Hinton}{2008}]{vandermaaten2008}
%   van~der Maaten, L. et Hinton, G. (2008).  \newblock Visualizing data using
%   t-{SNE}.  \newblock {\em Journal of Machine Learning Research}, 9:2579--2605.

% \bibitem[\protect\astroncite{Wattenberg et~al.}{2016}]{wattenberg2016}
%   Wattenberg, M., Viégas, F., et Johnson, I. (2016).  \newblock How to use
%   t-{SNE} effectively.  \newblock {\em Distill}.  \newblock
%   \url{http://distill.pub/2016/misread-tsne}.
% \end{thebibliography}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "sdd_2020_poly"
%%% End:
